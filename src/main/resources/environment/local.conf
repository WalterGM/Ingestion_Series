spark {
fs.gs.imp = "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem"
fs.AbstractFileSystem.gs.impl= "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"
fs.defaultFS= "gs://airflow-bucket/"
fs.gs.auth.service.account.enable="true"
google.cloud.auth.service.account.enable="true"
fs.gs.auth.type='SERVICE_ACCOUNT_JSON_KEYFILE'
; spark.sql.files.maxPartitionNum = "5"
spark.sql.sources.partitionOverwriteMode="dynamic"
spark.sql.shuffle.partitions ="10"
mapreduce.fileoutputcommitter.marksuccessfuljobs= "false"
spark.default.parallelism = "12"
spark.driver.memory = 8g
spark.executor.memory = 3g
}

